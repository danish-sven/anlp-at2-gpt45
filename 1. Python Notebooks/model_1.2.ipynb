{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "csv_file = './../3. Cleaned Data/20K.csv'\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original</th>\n",
       "      <th>word_count_original</th>\n",
       "      <th>corrected_fs</th>\n",
       "      <th>word_count_corrected_fs</th>\n",
       "      <th>word_count_diff</th>\n",
       "      <th>word_count_ratio</th>\n",
       "      <th>o_pos_tags</th>\n",
       "      <th>o_num_verbs</th>\n",
       "      <th>o_num_nouns</th>\n",
       "      <th>...</th>\n",
       "      <th>c_num_modal</th>\n",
       "      <th>num_verbs_diff</th>\n",
       "      <th>num_nouns_diff</th>\n",
       "      <th>num_adjs_diff</th>\n",
       "      <th>num_adv_diff</th>\n",
       "      <th>num_pronoun_diff</th>\n",
       "      <th>num_preposition_diff</th>\n",
       "      <th>num_conjunction_diff</th>\n",
       "      <th>num_article_diff</th>\n",
       "      <th>num_modal_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I will appreciate it if you correct my sentences.</td>\n",
       "      <td>9</td>\n",
       "      <td>I would appreciate it if you could correct my ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>[('I', 'PRP'), ('will', 'MD'), ('appreciate', ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's been getting colder these days here in Ja...</td>\n",
       "      <td>9</td>\n",
       "      <td>It's been getting colder lately here in Japan.</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>[('It', 'PRP'), (\"'s\", 'VBZ'), ('been', 'VBN')...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The summer weather in Japan is not agreeable t...</td>\n",
       "      <td>16</td>\n",
       "      <td>I find Japan's summer weather disagreeable bec...</td>\n",
       "      <td>13</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>[('The', 'DT'), ('summer', 'NN'), ('weather', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>So, as the winter is coming, I'm getting to fe...</td>\n",
       "      <td>11</td>\n",
       "      <td>So, as the winter is coming, I'm starting to f...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[('So', 'RB'), (',', ','), ('as', 'IN'), ('the...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>It is the very exciting season.</td>\n",
       "      <td>6</td>\n",
       "      <td>It is a very exciting season.</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[('It', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           original   \n",
       "0           0  I will appreciate it if you correct my sentences.  \\\n",
       "1           1  It's been getting colder these days here in Ja...   \n",
       "2           2  The summer weather in Japan is not agreeable t...   \n",
       "3           3  So, as the winter is coming, I'm getting to fe...   \n",
       "4           4                    It is the very exciting season.   \n",
       "\n",
       "   word_count_original                                       corrected_fs   \n",
       "0                    9  I would appreciate it if you could correct my ...  \\\n",
       "1                    9     It's been getting colder lately here in Japan.   \n",
       "2                   16  I find Japan's summer weather disagreeable bec...   \n",
       "3                   11  So, as the winter is coming, I'm starting to f...   \n",
       "4                    6                      It is a very exciting season.   \n",
       "\n",
       "   word_count_corrected_fs  word_count_diff  word_count_ratio   \n",
       "0                       10                1          1.111111  \\\n",
       "1                        8               -1          0.888889   \n",
       "2                       13               -3          0.812500   \n",
       "3                       11                0          1.000000   \n",
       "4                        6                0          1.000000   \n",
       "\n",
       "                                          o_pos_tags  o_num_verbs   \n",
       "0  [('I', 'PRP'), ('will', 'MD'), ('appreciate', ...            2  \\\n",
       "1  [('It', 'PRP'), (\"'s\", 'VBZ'), ('been', 'VBN')...            3   \n",
       "2  [('The', 'DT'), ('summer', 'NN'), ('weather', ...            1   \n",
       "3  [('So', 'RB'), (',', ','), ('as', 'IN'), ('the...            5   \n",
       "4  [('It', 'PRP'), ('is', 'VBZ'), ('the', 'DT'), ...            1   \n",
       "\n",
       "   o_num_nouns  ...  c_num_modal  num_verbs_diff  num_nouns_diff   \n",
       "0            1  ...            2               0               0  \\\n",
       "1            3  ...            0               0              -2   \n",
       "2            5  ...            0               0               0   \n",
       "3            1  ...            0               0               0   \n",
       "4            1  ...            0               0               0   \n",
       "\n",
       "   num_adjs_diff  num_adv_diff  num_pronoun_diff  num_preposition_diff   \n",
       "0              0             0                 0                     0  \\\n",
       "1              0             2                 0                     0   \n",
       "2              0            -1                 0                     0   \n",
       "3              0             0                 0                     0   \n",
       "4              0             0                 0                     0   \n",
       "\n",
       "  num_conjunction_diff  num_article_diff  num_modal_diff  \n",
       "0                    0                 0               1  \n",
       "1                    0                -1               0  \n",
       "2                    0                -1               0  \n",
       "3                    0                 0               0  \n",
       "4                    0                 0               0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names in the dataframe\n",
    "df.rename(columns = {'corrected_fs':'corrected'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert the train and validation DataFrames to Hugging Face's Dataset instances\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose the model\n",
    "model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model will run on CUDA\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Assign cuda to the device to use for training\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    " print(\"This model will run on CUDA\")\n",
    "elif  torch.backends.mps.is_available(): \n",
    " dev = \"mps:0\"\n",
    " print(\"This model will run on MPS\")\n",
    "else:\n",
    " dev = \"cpu\" \n",
    " print(\"This model will run on CPU\")\n",
    "device = torch.device(dev) \n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and the model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the tokenizer uses the same special tokens as GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to tokenize and format input-output pairs\n",
    "def tokenize_function(examples):\n",
    "    inputs = [f\"input: {orig} output: {corr}\" for orig, corr in zip(examples[\"original\"], examples[\"corrected\"])]\n",
    "    return tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Tokenize the train and validation data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mmap(tokenize_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, remove_columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39moriginal\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcorrected\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      3\u001b[0m val_dataset \u001b[39m=\u001b[39m val_dataset\u001b[39m.\u001b[39mmap(tokenize_function, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39moriginal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcorrected\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    564\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    565\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    566\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    526\u001b[0m }\n\u001b[1;32m    527\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    529\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    530\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/arrow_dataset.py:3004\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2996\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   2998\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   2999\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3002\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3003\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3004\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3005\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3006\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/arrow_dataset.py:3380\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3376\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   3377\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(shard\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   3378\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3380\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   3381\u001b[0m         batch,\n\u001b[1;32m   3382\u001b[0m         indices,\n\u001b[1;32m   3383\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(shard\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   3384\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   3385\u001b[0m     )\n\u001b[1;32m   3386\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3387\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3389\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/datasets/arrow_dataset.py:3261\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3259\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[1;32m   3260\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[0;32m-> 3261\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39;49mfn_args, \u001b[39m*\u001b[39;49madditional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfn_kwargs)\n\u001b[1;32m   3262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3263\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[1;32m   3264\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[1;32m   3265\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[50], line 7\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_function\u001b[39m(examples):\n\u001b[1;32m      6\u001b[0m     inputs \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{\u001b[39;00morig\u001b[39m}\u001b[39;00m\u001b[39m output: \u001b[39m\u001b[39m{\u001b[39;00mcorr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m orig, corr \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(examples[\u001b[39m\"\u001b[39m\u001b[39moriginal\u001b[39m\u001b[39m\"\u001b[39m], examples[\u001b[39m\"\u001b[39m\u001b[39mcorrected\u001b[39m\u001b[39m\"\u001b[39m])]\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(inputs, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2538\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2536\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2537\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2538\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2624\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2619\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2620\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2621\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2622\u001b[0m         )\n\u001b[1;32m   2623\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[0;32m-> 2624\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[1;32m   2625\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2626\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2627\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[1;32m   2628\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[1;32m   2629\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2630\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2631\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2632\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2633\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2634\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2635\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2636\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2637\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2638\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2639\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2640\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2641\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2642\u001b[0m     )\n\u001b[1;32m   2643\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2644\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[1;32m   2645\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[1;32m   2646\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2662\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2663\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2815\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2805\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2806\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2807\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2808\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2813\u001b[0m )\n\u001b[0;32m-> 2815\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m   2816\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[1;32m   2817\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2818\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2819\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2820\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2821\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2822\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2823\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2824\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2825\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2826\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2827\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2828\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2829\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2830\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2831\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2832\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2833\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/tokenization_utils.py:733\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids\n\u001b[0;32m--> 733\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(ids)\n\u001b[1;32m    734\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    735\u001b[0m input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/tokenization_utils.py:701\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    700\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenize(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 701\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    702\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[1;32m    703\u001b[0m     \u001b[39mif\u001b[39;00m is_split_into_words:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/tokenization_utils.py:579\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    577\u001b[0m ids \u001b[39m=\u001b[39m []\n\u001b[1;32m    578\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n\u001b[0;32m--> 579\u001b[0m     ids\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_token_to_id_with_added_voc(token))\n\u001b[1;32m    580\u001b[0m \u001b[39mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/tokenization_utils.py:588\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder:\n\u001b[1;32m    587\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madded_tokens_encoder[token]\n\u001b[0;32m--> 588\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_token_to_id(token)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py:310\u001b[0m, in \u001b[0;36mGPT2Tokenizer._convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert_token_to_id\u001b[39m(\u001b[39mself\u001b[39m, token):\n\u001b[1;32m    309\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mget(token, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munk_token))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tokenize the train and validation data\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['original', 'corrected'])\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['original', 'corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    warmup_steps=200,\n",
    "    logging_dir=\"logs\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom loss function to focus on the \"output\" tokens\n",
    "def custom_loss_function(outputs, labels):\n",
    "    shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    loss = torch.nn.CrossEntropyLoss()(shift_logits, shift_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Trainer class that inherits from the original Trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    \n",
    "    # Override the compute_loss method to use a custom loss function\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get the labels from the inputs dictionary and remove them from the inputs\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        \n",
    "        # Get the model outputs by passing the inputs to the model\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Extract the logits from the model outputs\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the correct dimensions for the shift_labels tensor\n",
    "        shift_labels = labels[..., 1:].reshape(-1)\n",
    "\n",
    "        # Reshape the shift_logits tensor to align with the dimensions of the shift_labels tensor\n",
    "        shift_logits = logits[..., :-1, :].reshape(-1, logits.size(-1))\n",
    "\n",
    "        loss = torch.nn.CrossEntropyLoss()(shift_logits, shift_labels)\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        \n",
    "        # Otherwise, just return the loss\n",
    "        return loss\n",
    "\n",
    "# Create the custom Trainer with the custom loss function\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    \n",
    "    # Use the DataCollatorForLanguageModeling to handle the data collation\n",
    "    # Set mlm=False, as we are not using masked language modeling\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and tokenizer\n",
    "output_dir = \"7. Models/20K/\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Then compress with this command: tar czvf trained_model.tar.gz trained_model/\n",
    "# Upload to git/drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "output_dir = \"7. Models/20K/\"\n",
    "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
